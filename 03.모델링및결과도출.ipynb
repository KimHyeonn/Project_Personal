{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ff0dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어 갯수:  100155\n",
      "vocab_size:  67158\n",
      "단어 사전 개수:  100155\n",
      "0    LG 유플러스 LTE 라우터 U 와이파이 쏙 출시\n",
      "1    삼성 증권 메 리츠 증권 유상증자 주가 영향 제한\n",
      "Name: title, dtype: object\n",
      "[[105, 3149, 3115, 43794, 1188, 6556, 4953, 78], [33, 1064, 2600, 6232, 1064, 11075, 1266, 514, 742]]\n",
      "문장 최대 길이: 40\n",
      "샘플: [[  105  3149  3115 43794  1188  6556  4953    78     0     0     0     0\n",
      "      0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "f = pd.read_csv('f.csv', encoding = 'utf-8').reset_index(drop=True)\n",
    "g = pd.read_csv('g.csv', encoding = 'utf-8').reset_index(drop=True)\n",
    "\n",
    "f.topic_idx = f.topic_idx.astype('int8')\n",
    "g.topic_idx = g.topic_idx.astype('int8')\n",
    "\n",
    "train_sentences = f['title']\n",
    "valid_sentences = g['title']\n",
    "data = f['title']\n",
    "train_labels = f['topic_idx']\n",
    "valid_labels = g['topic_idx']\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(data.astype(str))\n",
    "print('총 단어 갯수: ', len(tokenizer.word_index))\n",
    "\n",
    "# 2회 이상만 vocab_size에 포함\n",
    "def get_vocab_size(threshold):\n",
    "  cnt = 0\n",
    "  for x in tokenizer.word_counts.values():\n",
    "    if x >= threshold:\n",
    "      cnt = cnt + 1\n",
    "  return cnt\n",
    "vocab_size = get_vocab_size(2)\n",
    "print('vocab_size: ', vocab_size)\n",
    "\n",
    "oov_tok = '<OOV>' # 사전에 없는 단어\n",
    "vocab_size = get_vocab_size(2)\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_tok, num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data.astype(str))\n",
    "#print(tokenizer.word_index)\n",
    "print('단어 사전 개수: ', len(tokenizer.word_counts))\n",
    "\n",
    "print(train_sentences[:2])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences.astype(str))\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_sentences.astype(str))\n",
    "print(train_sequences[:2])\n",
    "\n",
    "# 문장의 최대 길이\n",
    "max_length = max(len(x) for x in train_sequences)\n",
    "print('문장 최대 길이:', max_length)\n",
    "\n",
    "# 문장 길이를 동일하게 맞춘다\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "train_padded = pad_sequences(train_sequences, \n",
    "                             truncating=trunc_type, \n",
    "                             padding=padding_type, \n",
    "                             maxlen=17)\n",
    "valid_padded = pad_sequences(valid_sequences, \n",
    "                             truncating=trunc_type, \n",
    "                             padding=padding_type, \n",
    "                             maxlen=17)\n",
    "train_labels = np.asarray(train_labels)\n",
    "valid_labels = np.asarray(valid_labels)\n",
    "print('샘플:', train_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f2ba0a2-ca73-4807-9068-9c66173eaab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4\n",
       "1         2\n",
       "2         1\n",
       "3         4\n",
       "4         5\n",
       "         ..\n",
       "140430    0\n",
       "140431    0\n",
       "140432    0\n",
       "140433    0\n",
       "140434    0\n",
       "Name: topic_idx, Length: 140435, dtype: int8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['topic_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd56c597-c950-4597-ad8b-ff9b9e3c7c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        3\n",
       "3        2\n",
       "4        5\n",
       "        ..\n",
       "13663    3\n",
       "13664    1\n",
       "13665    6\n",
       "13666    6\n",
       "13667    1\n",
       "Name: topic_idx, Length: 13668, dtype: int8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g['topic_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21236e6-2630-4e40-82dd-76cad3cf242b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    151972\n",
       "6    151281\n",
       "4    129084\n",
       "1    119466\n",
       "3    110215\n",
       "5     66835\n",
       "0     30625\n",
       "Name: topic_idx, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.topic_idx.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a0d579f0-c4d8-484d-b76d-0c64914b0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "y0 = np.where(f.topic_idx == 0, 1, 0)\n",
    "y00 = np.where(g.topic_idx == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b82b45f-731d-4951-90e4-c1cb89a5187c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c1e6961-c0cc-4839-a80b-35373268eb55",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[30,0] = 26720 is not in [0, 22417)\n\t [[node sequential_9/embedding_9/embedding_lookup (defined at <ipython-input-36-1b305f60dce5>:15) ]] [Op:__inference_train_function_266830]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_9/embedding_9/embedding_lookup:\n sequential_9/embedding_9/embedding_lookup/264442 (defined at C:\\Users\\admin\\Anaconda3\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1b305f60dce5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m   history_a = model_a.fit(train_padded, y0, epochs=1, batch_size=BATCH_SIZE,\n\u001b[1;32m---> 15\u001b[1;33m                     validation_data = (valid_padded, y00), shuffle = True)\n\u001b[0m\u001b[0;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  indices[30,0] = 26720 is not in [0, 22417)\n\t [[node sequential_9/embedding_9/embedding_lookup (defined at <ipython-input-36-1b305f60dce5>:15) ]] [Op:__inference_train_function_266830]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_9/embedding_9/embedding_lookup:\n sequential_9/embedding_9/embedding_lookup/264442 (defined at C:\\Users\\admin\\Anaconda3\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 7):\n",
    "  y0 = np.where(f.topic_idx == i, 1, 0)\n",
    "  y00 = np.where(g.topic_idx == i, 1, 0)\n",
    "\n",
    "  model_a = Sequential()\n",
    "  model_a.add(Embedding(22417, 128, input_length = 17))\n",
    "  model_a.add(Bidirectional(LSTM(128, return_sequences =True)))\n",
    "  model_a.add(tf.keras.layers.Dropout(0.2))\n",
    "  model_a.add(Dense(32, activation = 'tanh'))\n",
    "  model_a.add(Dense(8, activation = 'tanh'))\n",
    "  model_a.add(Dense(1, activation = 'sigmoid'))\n",
    "  model_a.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "  history_a = model_a.fit(train_padded, y0, epochs=1, batch_size=BATCH_SIZE,\n",
    "                    validation_data = (valid_padded, y00), shuffle = True)\n",
    "  print(history_a.history['val_loss'])\n",
    "  print(history_a.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa94657a-747f-47a8-b3f1-1a1ca629e87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[26,4] = 23313 is not in [0, 22417)\n\t [[node sequential_7/embedding_7/embedding_lookup (defined at <ipython-input-34-0512cba9b10d>:4) ]] [Op:__inference_train_function_254546]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_7/embedding_7/embedding_lookup:\n sequential_7/embedding_7/embedding_lookup/252158 (defined at C:\\Users\\admin\\Anaconda3\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-0512cba9b10d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my00\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                     shuffle = True)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  indices[26,4] = 23313 is not in [0, 22417)\n\t [[node sequential_7/embedding_7/embedding_lookup (defined at <ipython-input-34-0512cba9b10d>:4) ]] [Op:__inference_train_function_254546]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node sequential_7/embedding_7/embedding_lookup:\n sequential_7/embedding_7/embedding_lookup/252158 (defined at C:\\Users\\admin\\Anaconda3\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history0 = model_a.fit(train_padded, y0, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_data = (valid_padded, y00), \n",
    "                    callbacks=[checkpoint, early_stop],\n",
    "                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b57a6e1-059d-455f-9b34-49206ff47c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0268fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_label tensor: (759478, 7)\n",
      "Shape of valid_label tensor: (13679, 7)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 17, 128)           8596224   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 17, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 8,732,167\n",
      "Trainable params: 8,732,167\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구축\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, SimpleRNN\n",
    "\n",
    "# callback : 가장 좋은 loss의 가중치 저장\n",
    "checkpoint_path = 'best_performed_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                save_weights_only=True,\n",
    "                                                save_best_only=True,\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1)\n",
    "\n",
    "# 얼리스탑 // 3회로\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# 라벨링 원핫\n",
    "Y = pd.get_dummies(train_labels).values\n",
    "print('Shape of train_label tensor:', Y.shape)\n",
    "Y_val = pd.get_dummies(valid_labels).values\n",
    "print('Shape of valid_label tensor:', Y_val.shape)\n",
    "\n",
    "# 러닝레이트 0.001 이 default인데, 적은 에포크에서도 과적합 발생하니 줄여보자\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam')\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(67158, 128, input_length=17))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2))\n",
    "model.add(Dense(32, activation = 'tanh'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292250df",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 3442/23734 [===>..........................] - ETA: 36:09 - loss: 1.0302 - accuracy: 0.6382"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-41c1029f40cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                     shuffle = True)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, Y, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_data = (valid_padded, Y_val), \n",
    "                    callbacks=[checkpoint, early_stop],\n",
    "                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2ec7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 33, 128)           6377472   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 6,513,415\n",
      "Trainable params: 6,513,415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## epoch 6 정도가 과적합 막고 베스트로 보임, 새로이 모델링\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(31547, 128, input_length=17))\n",
    "model1.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "model1.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model1.add(Dense(32, activation = 'relu'))\n",
    "model1.add(tf.keras.layers.Dropout(0.2))\n",
    "model1.add(Dense(7, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce66b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "4816/4816 [==============================] - 642s 133ms/step - loss: 1.3232 - accuracy: 0.4848\n",
      "Epoch 2/6\n",
      "4816/4816 [==============================] - 621s 129ms/step - loss: 0.8653 - accuracy: 0.7137\n",
      "Epoch 3/6\n",
      "4816/4816 [==============================] - 601s 125ms/step - loss: 0.7390 - accuracy: 0.7635\n",
      "Epoch 4/6\n",
      "4816/4816 [==============================] - 606s 126ms/step - loss: 0.6716 - accuracy: 0.7875\n",
      "Epoch 5/6\n",
      "4816/4816 [==============================] - 603s 125ms/step - loss: 0.6169 - accuracy: 0.8045\n",
      "Epoch 6/6\n",
      "4816/4816 [==============================] - 605s 126ms/step - loss: 0.5770 - accuracy: 0.8173\n"
     ]
    }
   ],
   "source": [
    "# train/valid set 합쳐서 4회 학습\n",
    "\n",
    "X = np.concatenate((train_padded, valid_padded))\n",
    "y = np.concatenate((Y, Y_val))\n",
    "\n",
    "history1 = model1.fit(X, y, epochs=6, batch_size=32,\n",
    "                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "712aa18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission 예측\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "submit = pd.read_csv('test_data.csv', encoding = 'utf-8', index_col = 0)\n",
    "\n",
    "submit.title = submit.title.str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ一-龥]', ' ')\n",
    "submit.reset_index(drop = True, inplace = True)\n",
    "okt=Okt()\n",
    "h = pd.DataFrame()\n",
    "for i in range(len(submit.title)):\n",
    "    c = okt.pos(submit.title[i])\n",
    "    d = [y for (y, x) in c if (x=='Noun')|(x == 'Alpha')|(x=='Foreign')]\n",
    "\n",
    "    e = ''\n",
    "    for j in range(len(d)):\n",
    "        if j < (len(d) - 1):\n",
    "            e += d[j]+' '\n",
    "        else:\n",
    "            e += d[j]\n",
    "    h = pd.concat([h, pd.DataFrame(np.array([e]).reshape(1,1))])\n",
    "h.columns = ['title']\n",
    "h.reset_index(drop=True, inplace=True)\n",
    "\n",
    "submit_sentences = h['title']\n",
    "submit_X = tokenizer.texts_to_sequences(submit_sentences)\n",
    "\n",
    "submit_padded = pad_sequences(submit_X, \n",
    "                              truncating=trunc_type, \n",
    "                              padding=padding_type, \n",
    "                              maxlen=17)\n",
    "\n",
    "\n",
    "result = model1.predict(submit_padded, batch_size = BATCH_SIZE)\n",
    "\n",
    "submission = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    submission.append(np.argmax(result[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddffe948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장\n",
    "subm = pd.read_csv('sample_submission.csv')\n",
    "subm.topic_idx = submission\n",
    "subm.to_csv('submission0902.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c883a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0df892c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어 갯수:  22417\n",
      "vocab_size:  13789\n",
      "단어 사전 개수:  22417\n",
      "0        터키 국방 쿠르드 민병대 예정 철수 중\n",
      "1    학부모 방과후 필요 명 중 명 초등 교실 선호\n",
      "Name: title, dtype: object\n",
      "[[80, 852, 840, 3541, 853, 1056, 33], [2382, 5746, 426, 5, 33, 5, 2124, 1565, 2201]]\n",
      "문장 최대 길이: 17\n",
      "샘플: [[  80  852  840 3541  853 1056   33    0    0    0    0    0    0    0\n",
      "     0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "## 크롤링 제외, 원 트레이닝 세트만 가지고\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "f = pd.read_csv('f1.csv', encoding = 'utf-8').reset_index(drop=True)\n",
    "g = pd.read_csv('g1.csv', encoding = 'utf-8').reset_index(drop=True)\n",
    "\n",
    "f.topic_idx = f.topic_idx.astype('int8')\n",
    "g.topic_idx = g.topic_idx.astype('int8')\n",
    "\n",
    "train_sentences = f['title']\n",
    "valid_sentences = g['title']\n",
    "data = f['title']\n",
    "train_labels = f['topic_idx']\n",
    "valid_labels = g['topic_idx']\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(data)\n",
    "print('총 단어 갯수: ', len(tokenizer.word_index))\n",
    "\n",
    "# 2회 이상만 vocab_size에 포함\n",
    "def get_vocab_size(threshold):\n",
    "  cnt = 0\n",
    "  for x in tokenizer.word_counts.values():\n",
    "    if x >= threshold:\n",
    "      cnt = cnt + 1\n",
    "  return cnt\n",
    "vocab_size = get_vocab_size(2)\n",
    "print('vocab_size: ', vocab_size)\n",
    "\n",
    "oov_tok = '<OOV>' # 사전에 없는 단어\n",
    "vocab_size = get_vocab_size(2)\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=oov_tok, num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(data)\n",
    "#print(tokenizer.word_index)\n",
    "print('단어 사전 개수: ', len(tokenizer.word_counts))\n",
    "\n",
    "print(train_sentences[:2])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "valid_sequences = tokenizer.texts_to_sequences(valid_sentences)\n",
    "print(train_sequences[:2])\n",
    "\n",
    "# 문장의 최대 길이\n",
    "max_length = max(len(x) for x in train_sequences)\n",
    "print('문장 최대 길이:', max_length)\n",
    "\n",
    "# 문장 길이를 동일하게 맞춘다\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "train_padded = pad_sequences(train_sequences, \n",
    "                             truncating=trunc_type, \n",
    "                             padding=padding_type, \n",
    "                             maxlen=max_length)\n",
    "valid_padded = pad_sequences(valid_sequences, \n",
    "                             truncating=trunc_type, \n",
    "                             padding=padding_type, \n",
    "                             maxlen=max_length)\n",
    "train_labels = np.asarray(train_labels)\n",
    "valid_labels = np.asarray(valid_labels)\n",
    "print('샘플:', train_padded[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1afbc84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_label tensor: (31866, 7)\n",
      "Shape of valid_label tensor: (13668, 7)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 17, 128)           2869376   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_6 (Spatial (None, 17, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 3,005,319\n",
      "Trainable params: 3,005,319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구축\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, SimpleRNN\n",
    "\n",
    "# callback : 가장 좋은 loss의 가중치 저장\n",
    "checkpoint_path = 'best_performed_model.ckpt'\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n",
    "                                                save_weights_only=True,\n",
    "                                                save_best_only=True,\n",
    "                                                monitor='val_loss',\n",
    "                                                verbose=1)\n",
    "\n",
    "# 얼리스탑 // 3회로\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# 라벨링 원핫\n",
    "Y = pd.get_dummies(train_labels).values\n",
    "print('Shape of train_label tensor:', Y.shape)\n",
    "Y_val = pd.get_dummies(valid_labels).values\n",
    "print('Shape of valid_label tensor:', Y_val.shape)\n",
    "\n",
    "# 러닝레이트 0.001 이 default인데, 적은 에포크에서도 과적합 발생하니 줄여보자\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(22417, 128, input_length=17))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50b5b720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "996/996 [==============================] - 80s 77ms/step - loss: 1.3210 - accuracy: 0.4666 - val_loss: 0.7843 - val_accuracy: 0.7256\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.78432, saving model to best_performed_model.ckpt\n",
      "Epoch 2/20\n",
      "996/996 [==============================] - 78s 78ms/step - loss: 0.6886 - accuracy: 0.7702 - val_loss: 0.5904 - val_accuracy: 0.8094\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.78432 to 0.59038, saving model to best_performed_model.ckpt\n",
      "Epoch 3/20\n",
      "996/996 [==============================] - 74s 74ms/step - loss: 0.4981 - accuracy: 0.8469 - val_loss: 0.5646 - val_accuracy: 0.8172\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.59038 to 0.56457, saving model to best_performed_model.ckpt\n",
      "Epoch 4/20\n",
      "996/996 [==============================] - 75s 75ms/step - loss: 0.3985 - accuracy: 0.8786 - val_loss: 0.5464 - val_accuracy: 0.8276\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56457 to 0.54638, saving model to best_performed_model.ckpt\n",
      "Epoch 5/20\n",
      "996/996 [==============================] - 76s 77ms/step - loss: 0.3428 - accuracy: 0.8984 - val_loss: 0.5686 - val_accuracy: 0.8273\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.54638\n",
      "Epoch 6/20\n",
      "996/996 [==============================] - 77s 78ms/step - loss: 0.2898 - accuracy: 0.9137 - val_loss: 0.5966 - val_accuracy: 0.8276\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.54638\n",
      "Epoch 7/20\n",
      "996/996 [==============================] - 79s 79ms/step - loss: 0.2535 - accuracy: 0.9242 - val_loss: 0.6083 - val_accuracy: 0.8234\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.54638\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_padded, Y, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                    validation_data = (valid_padded, Y_val), \n",
    "                    callbacks=[checkpoint, early_stop],\n",
    "                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cdf5739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 17, 128)           2869376   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, 17, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 7)                 231       \n",
      "=================================================================\n",
      "Total params: 3,005,319\n",
      "Trainable params: 3,005,319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/4\n",
      "1423/1423 [==============================] - 96s 65ms/step - loss: 0.9773 - accuracy: 0.6437\n",
      "Epoch 2/4\n",
      "1423/1423 [==============================] - 99s 70ms/step - loss: 0.6205 - accuracy: 0.80160s - loss: 0.6205 - accuracy: \n",
      "Epoch 3/4\n",
      "1423/1423 [==============================] - 104s 73ms/step - loss: 0.4979 - accuracy: 0.8482s - loss: 0.4978 - accuracy: \n",
      "Epoch 4/4\n",
      "1423/1423 [==============================] - 95s 67ms/step - loss: 0.4074 - accuracy: 0.8803\n"
     ]
    }
   ],
   "source": [
    "## epoch 4 정도가 과적합 막고 베스트로 보임, 새로이 모델링\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(22417, 128, input_length=17))\n",
    "model1.add(tf.keras.layers.SpatialDropout1D(0.2))\n",
    "model1.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model1.add(Dense(32, activation = 'relu'))\n",
    "model1.add(tf.keras.layers.Dropout(0.2))\n",
    "model1.add(Dense(7, activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model1.summary()\n",
    "\n",
    "# train/valid set 합쳐서 4회 학습\n",
    "\n",
    "X = np.concatenate((train_padded, valid_padded))\n",
    "y = np.concatenate((Y, Y_val))\n",
    "\n",
    "history1 = model1.fit(X, y, epochs=4, batch_size=32,\n",
    "                    shuffle = True)\n",
    "\n",
    "# submission 예측\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "submit = pd.read_csv('test_data.csv', encoding = 'utf-8', index_col = 0)\n",
    "\n",
    "submit.title = submit.title.str.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ一-龥]', ' ')\n",
    "submit.reset_index(drop = True, inplace = True)\n",
    "okt=Okt()\n",
    "h = pd.DataFrame()\n",
    "for i in range(len(submit.title)):\n",
    "    c = okt.pos(submit.title[i])\n",
    "    d = [y for (y, x) in c if (x=='Noun')|(x == 'Alpha')|(x=='Foreign')]\n",
    "\n",
    "    e = ''\n",
    "    for j in range(len(d)):\n",
    "        if j < (len(d) - 1):\n",
    "            e += d[j]+' '\n",
    "        else:\n",
    "            e += d[j]\n",
    "    h = pd.concat([h, pd.DataFrame(np.array([e]).reshape(1,1))])\n",
    "h.columns = ['title']\n",
    "h.reset_index(drop=True, inplace=True)\n",
    "\n",
    "submit_sentences = h['title']\n",
    "submit_X = tokenizer.texts_to_sequences(submit_sentences)\n",
    "\n",
    "submit_padded = pad_sequences(submit_X, \n",
    "                              truncating=trunc_type, \n",
    "                              padding=padding_type, \n",
    "                              maxlen=max_length)\n",
    "\n",
    "\n",
    "result = model1.predict(submit_padded, batch_size = BATCH_SIZE)\n",
    "\n",
    "submission = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    submission.append(np.argmax(result[i]))\n",
    "    \n",
    "    \n",
    "# 결과 저장\n",
    "subm = pd.read_csv('sample_submission.csv')\n",
    "subm.topic_idx = submission\n",
    "subm.to_csv('submission0901_onlyTrain.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed34cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
