{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_trial1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d68bee92a07c43a699270d451a58d1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0d974aa0b5a246d185180dd82f0f4aaf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0e9e4de64f1b4fce8700994ed47948eb",
              "IPY_MODEL_fae2cc7139f3424eaa9a994ff831f21a",
              "IPY_MODEL_2dd92274974f42159f8db80bce0fa603"
            ]
          }
        },
        "0d974aa0b5a246d185180dd82f0f4aaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e9e4de64f1b4fce8700994ed47948eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4c5589dda75d4ae1b84ef4c33ae2f739",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_95cda5160fe3482eb03c4e8a6da0b935"
          }
        },
        "fae2cc7139f3424eaa9a994ff831f21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c71ac1f73c0942bcad494c8332eb9035",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 368792146,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 368792146,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_828e4d507a6a4233976253de763d9361"
          }
        },
        "2dd92274974f42159f8db80bce0fa603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c5c953d47b7e47da8c5fb58779dd7dce",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 369M/369M [00:09&lt;00:00, 40.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3be1649751c8410fb49700f29128a43f"
          }
        },
        "4c5589dda75d4ae1b84ef4c33ae2f739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "95cda5160fe3482eb03c4e8a6da0b935": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c71ac1f73c0942bcad494c8332eb9035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "828e4d507a6a4233976253de763d9361": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5c953d47b7e47da8c5fb58779dd7dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3be1649751c8410fb49700f29128a43f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyLSPSqNt51Z"
      },
      "source": [
        "!pip install hanja\n",
        "!pip install konlpy\n",
        "!pip install transformers\n",
        "!pip install tensorflow_addons\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY3UOGINtWNC"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import hanja\n",
        "from hanja import hangul\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold\n",
        "from tensorflow.keras.models import clone_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONVUqsVAtlhi"
      },
      "source": [
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from shutil import copyfile\n",
        " \n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "logger = logging.getLogger(__name__)\n",
        " \n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"tokenizer_78b3253a26.model\",\n",
        "                     \"vocab_txt\": \"vocab.txt\"}\n",
        " \n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/tokenizer_78b3253a26.model\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/tokenizer_78b3253a26.model\"\n",
        "    },\n",
        "    \"vocab_txt\": {\n",
        "        \"monologg/kobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert/vocab.txt\",\n",
        "        \"monologg/kobert-lm\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/kobert-lm/vocab.txt\",\n",
        "        \"monologg/distilkobert\": \"https://s3.amazonaws.com/models.huggingface.co/bert/monologg/distilkobert/vocab.txt\"\n",
        "    }\n",
        "}\n",
        " \n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"monologg/kobert\": 512,\n",
        "    \"monologg/kobert-lm\": 512,\n",
        "    \"monologg/distilkobert\": 512\n",
        "}\n",
        " \n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"monologg/kobert\": {\"do_lower_case\": False},\n",
        "    \"monologg/kobert-lm\": {\"do_lower_case\": False},\n",
        "    \"monologg/distilkobert\": {\"do_lower_case\": False}\n",
        "}\n",
        " \n",
        "SPIECE_UNDERLINE = u'▁'\n",
        " \n",
        " \n",
        "class KoBertTokenizer(PreTrainedTokenizer):\n",
        "    \"\"\"\n",
        "        SentencePiece based tokenizer. Peculiarities:\n",
        "            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n",
        "    \"\"\"\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        " \n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_file,\n",
        "            vocab_txt,\n",
        "            do_lower_case=False,\n",
        "            remove_space=True,\n",
        "            keep_accents=False,\n",
        "            unk_token=\"[UNK]\",\n",
        "            sep_token=\"[SEP]\",\n",
        "            pad_token=\"[PAD]\",\n",
        "            cls_token=\"[CLS]\",\n",
        "            mask_token=\"[MASK]\",\n",
        "            **kwargs):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs\n",
        "        )\n",
        " \n",
        "        # Build vocab\n",
        "        self.token2idx = dict()\n",
        "        self.idx2token = []\n",
        "        with open(vocab_txt, 'r', encoding='utf-8') as f:\n",
        "            for idx, token in enumerate(f):\n",
        "                token = token.strip()\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token.append(token)\n",
        " \n",
        "        #self.max_len_single_sentence = self.max_len - 2  \n",
        "        #self.max_len_sentences_pair = self.max_len - 3  \n",
        " \n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        " \n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.remove_space = remove_space\n",
        "        self.keep_accents = keep_accents\n",
        "        self.vocab_file = vocab_file\n",
        "        self.vocab_txt = vocab_txt\n",
        " \n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(vocab_file)\n",
        " \n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.idx2token)\n",
        " \n",
        "    def __getstate__(self):\n",
        "        state = self.__dict__.copy()\n",
        "        state[\"sp_model\"] = None\n",
        "        return state\n",
        " \n",
        "    def __setstate__(self, d):\n",
        "        self.__dict__ = d\n",
        "        try:\n",
        "            import sentencepiece as spm\n",
        "        except ImportError:\n",
        "            logger.warning(\"You need to install SentencePiece to use KoBertTokenizer: https://github.com/google/sentencepiece\"\n",
        "                           \"pip install sentencepiece\")\n",
        "        self.sp_model = spm.SentencePieceProcessor()\n",
        "        self.sp_model.Load(self.vocab_file)\n",
        " \n",
        "    def preprocess_text(self, inputs):\n",
        "        if self.remove_space:\n",
        "            outputs = \" \".join(inputs.strip().split())\n",
        "        else:\n",
        "            outputs = inputs\n",
        "        outputs = outputs.replace(\"``\", '\"').replace(\"''\", '\"')\n",
        " \n",
        "        if not self.keep_accents:\n",
        "            outputs = unicodedata.normalize('NFKD', outputs)\n",
        "            outputs = \"\".join([c for c in outputs if not unicodedata.combining(c)])\n",
        "        if self.do_lower_case:\n",
        "            outputs = outputs.lower()\n",
        " \n",
        "        return outputs\n",
        " \n",
        "    def _tokenize(self, text, return_unicode=True, sample=False):\n",
        "        \"\"\" Tokenize a string. \"\"\"\n",
        "        text = self.preprocess_text(text)\n",
        " \n",
        "        if not sample:\n",
        "            pieces = self.sp_model.EncodeAsPieces(text)\n",
        "        else:\n",
        "            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n",
        "        new_pieces = []\n",
        "        for piece in pieces:\n",
        "            if len(piece) > 1 and piece[-1] == str(\",\") and piece[-2].isdigit():\n",
        "                cur_pieces = self.sp_model.EncodeAsPieces(piece[:-1].replace(SPIECE_UNDERLINE, \"\"))\n",
        "                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n",
        "                    if len(cur_pieces[0]) == 1:\n",
        "                        cur_pieces = cur_pieces[1:]\n",
        "                    else:\n",
        "                        cur_pieces[0] = cur_pieces[0][1:]\n",
        "                cur_pieces.append(piece[-1])\n",
        "                new_pieces.extend(cur_pieces)\n",
        "            else:\n",
        "                new_pieces.append(piece)\n",
        " \n",
        "        return new_pieces\n",
        " \n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str/unicode) in an id using the vocab. \"\"\"\n",
        "        return self.token2idx.get(token, self.token2idx[self.unk_token])\n",
        " \n",
        "    def _convert_id_to_token(self, index, return_unicode=True):\n",
        "        \"\"\"Converts an index (integer) in a token (string/unicode) using the vocab.\"\"\"\n",
        "        return self.idx2token[index]\n",
        " \n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens (strings for sub-words) in a single string.\"\"\"\n",
        "        out_string = \"\".join(tokens).replace(SPIECE_UNDERLINE, \" \").strip()\n",
        "        return out_string\n",
        " \n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A RoBERTa sequence has the following format:\n",
        "            single sequence: [CLS] X [SEP]\n",
        "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        " \n",
        "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
        "        Args:\n",
        "            token_ids_0: list of ids (must not contain special tokens)\n",
        "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
        "                for sequence pairs\n",
        "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
        "                special tokens for the model\n",
        "        Returns:\n",
        "            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
        "        \"\"\"\n",
        " \n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        " \n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        " \n",
        "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
        "        | first sequence    | second sequence\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        " \n",
        "    def save_vocabulary(self, save_directory):\n",
        "        \"\"\" Save the sentencepiece vocabulary (copy original file) and special tokens file\n",
        "            to a directory.\n",
        "        \"\"\"\n",
        "        if not os.path.isdir(save_directory):\n",
        "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
        "            return\n",
        " \n",
        "        # 1. Save sentencepiece model\n",
        "        out_vocab_model = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        " \n",
        "        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_model):\n",
        "            copyfile(self.vocab_file, out_vocab_model)\n",
        " \n",
        "        # 2. Save vocab.txt\n",
        "        index = 0\n",
        "        out_vocab_txt = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_txt\"])\n",
        "        with open(out_vocab_txt, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.token2idx.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(out_vocab_txt)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        " \n",
        "        return out_vocab_model, out_vocab_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw2i0tartnWt"
      },
      "source": [
        "train = pd.concat([pd.read_csv('f.csv'), pd.read_csv('g.csv')], axis = 0).reset_index(drop=True)\n",
        "test = pd.read_csv(\"test_data.csv\")\n",
        "submission = pd.read_csv(\"sample_submission.csv\")\n",
        "topic_dict = pd.read_csv(\"topic_dict.csv\")\n",
        "\n",
        "STOPWORDSPATH =\"stopwords.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emSbnbJTtsxt"
      },
      "source": [
        "## preprocessing\n",
        "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
        "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
        "\n",
        "def clean_punc(text, punct, mapping):\n",
        "    for p in mapping:\n",
        "        text = text.replace(p, mapping[p])\n",
        "    \n",
        "    for p in punct:\n",
        "        text = text.replace(p, f' {p} ')\n",
        "    \n",
        "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
        "    for s in specials:\n",
        "        text = text.replace(s, specials[s])\n",
        "    \n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "cleaned_train_corpus = []\n",
        "cleaned_test_corpus = []\n",
        "train.title = train.title.apply(lambda x : hanja.translate(x, 'substitution'))\n",
        "test.title = test.title.apply(lambda x : hanja.translate(x, 'substitution'))\n",
        "\n",
        "for sent in train['title']:\n",
        "    cleaned_train_corpus.append(clean_punc(sent, punct, punct_mapping))\n",
        "    \n",
        "for sent in test['title']:\n",
        "    cleaned_test_corpus.append(clean_punc(sent, punct, punct_mapping))\n",
        "\n",
        "\n",
        "def clean_text(texts):\n",
        "    corpus = []\n",
        "    for i in range(0, len(texts)):\n",
        "        texts[i] = texts[i].replace(\"外人\",\"외국인\")\n",
        "        texts[i] = texts[i].replace(\"日\",\"일본\")\n",
        "        texts[i] = texts[i].replace(\"美\",\"미국\")\n",
        "        texts[i] = texts[i].replace(\"北\",\"북한\")\n",
        "        texts[i] = texts[i].replace(\"英\",\"영국\")\n",
        "        texts[i] = texts[i].replace(\"中\",\"중국\")\n",
        "        texts[i] = texts[i].replace(\"與\",\"여당\")\n",
        "        texts[i] = texts[i].replace(\"靑\",\"청와대\")\n",
        "        texts[i] = texts[i].replace(\"野\",\"야당\")\n",
        "        texts[i] = texts[i].replace(\"伊\",\"이탈리아\")\n",
        "        texts[i] = texts[i].replace(\"韓\",\"한국\")\n",
        "        texts[i] = texts[i].replace(\"南\",\"한국\")\n",
        "        texts[i] = texts[i].replace(\"獨\",\"독일\")\n",
        "        texts[i] = texts[i].replace(\"佛\",\"프랑스\")\n",
        "        texts[i] = texts[i].replace(\"檢\",\"검찰\")\n",
        "        texts[i] = texts[i].replace(\"銀\",\"은행\")\n",
        "        texts[i] = texts[i].replace(\"亞\",\"아시아\")\n",
        "        texts[i] = texts[i].replace(\"人\",\"사람\")\n",
        "        texts[i] = texts[i].replace(\"孫\",\"손혜원\")\n",
        "        texts[i] = texts[i].replace(\"企\",\"기업\")\n",
        "        texts[i] = texts[i].replace(\"前\",\"이전\")\n",
        "        texts[i] = texts[i].replace(\"反\",\"반대\")\n",
        "        texts[i] = texts[i].replace(\"安\",\"안철수\")\n",
        "        texts[i] = texts[i].replace(\"展\",\"전시회\")\n",
        "        texts[i] = texts[i].replace(\"故\",\"사망\")\n",
        "        texts[i] = texts[i].replace(\"文\",\"문재인\")\n",
        "        texts[i] = texts[i].replace(\"新\",\"새로운\")\n",
        "        texts[i] = texts[i].replace(\"曺\",\"조국\")\n",
        "        texts[i] = texts[i].replace(\"朴\",\"박근혜\")\n",
        "        texts[i] = texts[i].replace(\"株\",\"주식\")\n",
        "        texts[i] = texts[i].replace(\"男\",\"남자\")\n",
        "        texts[i] = texts[i].replace(\"硏\",\"연구\")\n",
        "        texts[i] = texts[i].replace(\"車\",\"자동차\")\n",
        "        texts[i] = texts[i].replace(\"軍\",\"군대\")\n",
        "        texts[i] = texts[i].replace(\"重\",\"중공업\")       \n",
        "\n",
        "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
        "        review = re.sub(r'1보','', str(review))\n",
        "        review = re.sub(r'\\d+','', str(review))# remove number\n",
        "        review = re.sub(r'→','에서 ', str(review))\n",
        "        review = re.sub(r'…',' ', str(review))\n",
        "        review = re.sub(r'NYT','뉴욕 타임스', str(review))\n",
        "        review = re.sub(r'KAIST','카이스트', str(review))\n",
        "        review = re.sub(r'WMO','세계 기상 기구', str(review))\n",
        "        review = re.sub(r'KBL','한국 프로 농구', str(review))\n",
        "        review = re.sub(r'UAE','아랍에미리트', str(review))\n",
        "        review = re.sub(r'EU','유럽 연합', str(review))\n",
        "        review = re.sub(r'NBA','농구 연맹', str(review))\n",
        "        review = re.sub(r'CIA','중앙정보국', str(review))\n",
        "        review = re.sub(r'ECB','유럽 중앙 은행', str(review))\n",
        "        review = re.sub(r'AFC','아시아 축구 연맹', str(review))\n",
        "        review = re.sub(r'ITU','국제전기통신연합', str(review))\n",
        "        review = re.sub(r'MVP','최우수 선수', str(review))\n",
        "        #review = re.sub(r'MB','이명박', str(review))\n",
        "        review = re.sub(r'APEC','아시아 태평량 경제협력체', str(review))\n",
        "        review = re.sub(r'PSG','파리 셍제르망', str(review))\n",
        "        review = re.sub(r'IMO','국제해사기구', str(review))\n",
        "        review = re.sub(r'MLB','프로 야구 리그 ', str(review))\n",
        "        review = re.sub(r'MOU','양해각서', str(review))\n",
        "        review = re.sub(r'FA','자유계약선수제도', str(review))\n",
        "        review = re.sub(r'EPL','잉글랜드프리미어리그', str(review))\n",
        "        review = re.sub(r'KBO','한국야구위원회', str(review))\n",
        "        review = re.sub(r'IPU','국제 의회 연맹', str(review))\n",
        "        review = re.sub(r'AG','아시안게임', str(review))\n",
        "        review = re.sub(r'PS','포스트시즌', str(review))\n",
        "        review = re.sub(r'PO','플레이오프', str(review))\n",
        "        #review = re.sub(r'닷컴','사이트', str(review))\n",
        "        #review = re.sub(r'OUT','방출', str(review))\n",
        "        #review = re.sub(r'IN','영입', str(review))\n",
        "        review = re.sub(r'TPP',' 환태평양 경제 동반자협정', str(review))\n",
        "        review = re.sub(r'EAS','동아시아 정상회의', str(review))\n",
        "        review = re.sub(r'DC','', str(review))\n",
        "        #review = re.sub(r'①','', str(review))\n",
        "        #review = re.sub(r'②','', str(review))\n",
        "        #review = re.sub(r'⑤','', str(review))\n",
        "        review = re.sub(r'·',' 및 ', str(review))\n",
        "        #sent = re.sub(r'G20','', str(sent))\n",
        "        review = re.sub(r'↑','상승 ', str(review))\n",
        "        review = re.sub(r'↓','하락 ', str(review))\n",
        "        review = re.sub(r'ITF','국제태권도연맹 ', str(review))\n",
        "        review = re.sub(r'IS','이슬람 ', str(review))\n",
        "        #review = re.sub(r'러','러시아 ', str(review))\n",
        "        review = re.sub(r'W농구','한국여자농구', str(review))\n",
        "        review = re.sub(r'C팰리스','크리스탈팰리스', str(review))\n",
        "        review = re.sub(r'SLBM','잠수함발사탄도미사일', str(review))\n",
        "        review = re.sub(r'VNL','배구네이션스리그', str(review))\n",
        "        #sent = re.sub(r'D','하루전', str(sent))\n",
        "        review = re.sub(r'LA타임스','로스엔젤레스타임스', str(review))\n",
        "        review = re.sub(r'V리그','배구리그', str(review))\n",
        "        review = re.sub(r'KOVO','한국배구연맹', str(review))\n",
        "        review = re.sub(r'ℓ','리터', str(review))\n",
        "        review = re.sub(r'SUN','선동열', str(review))\n",
        "        review = re.sub(r'WSJ',' 월스트리트 저널', str(review))\n",
        "        review = re.sub(r'ERA',' 평균자책점', str(review))\n",
        "        review = re.sub(r'IoT',' 사물인터넷', str(review))\n",
        "        review = re.sub(r'QS',' 선발 3자책점 투구', str(review))\n",
        "        review = re.sub(r'NL','내셔널리그', str(review))\n",
        "        review = re.sub(r'UFG20','한미 합동 군사', str(review))\n",
        "        review = re.sub(r'F35','전투기', str(review))\n",
        "        review = re.sub(r'WP','워싱턴포스트', str(review))\n",
        "        review = re.sub(r'TK','대구와 경북', str(review))\n",
        "        review = re.sub(r'ACL','아시아축구연맹 챔피언스리그', str(review))\n",
        "        review = re.sub(r'IT','정보기술', str(review))\n",
        "        review = re.sub(r'AI','인공지능', str(review))\n",
        "        review = re.sub(r'TF','태스크포스', str(review))\n",
        "        review = re.sub(r'ML','메이저리그', str(review))\n",
        "        review = re.sub(r'FC','축구 클럽', str(review))\n",
        "        #review = re.sub(r'SI','스포츠 일러스트레이티드', str(review))\n",
        "        review = re.sub(r'㈜','', str(review))\n",
        "        review = re.sub(r'MS','마이크로소프트', str(review))\n",
        "        review = re.sub(r'SNS','소셜 네트워크 서비스', str(review))\n",
        "        review = re.sub(r'B52',' 전투기', str(review))\n",
        "        review = re.sub(r'VR','가상현실', str(review))\n",
        "        review = re.sub(r'ELB','주가 연계 파생상품', str(review))\n",
        "        review = re.sub(r'CES','국제전자제품박람회', str(review))\n",
        "        review = re.sub(r'NPL','부실채권', str(review))\n",
        "        review = re.sub(r'IPO','기업공개', str(review))\n",
        "        review = re.sub(r'ERA','방어율', str(review))\n",
        "        review = re.sub(r'MWC','모바일 산업 박람회', str(review))\n",
        "        review = re.sub(r'NSC','국가안전보장회의', str(review))\n",
        "        review = review.lower() #lower case\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
        "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
        "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
        "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
        "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
        "        #review = re.sub(\"[一-龥]\",'', review)\n",
        "\n",
        "        review = review.replace('[^A-Za-z가-힣ㄱ-ㅎㅏ-ㅣ一-龥]', ' ')\n",
        "        corpus.append(review)\n",
        "    return corpus\n",
        "\n",
        "basic_preprocessed_train_corpus = clean_text(cleaned_train_corpus)\n",
        "basic_preprocessed_test_corpus = clean_text(cleaned_test_corpus)\n",
        "\n",
        "\n",
        "stopwords = []\n",
        "# with open(STOPWORDSPATH) as f:\n",
        "#     for line in f:\n",
        "#         stopwords.append(line.strip())\n",
        "\n",
        "removed_stopword_train_corpus = []\n",
        "removed_stopword_test_corpus = []\n",
        "\n",
        "for tagged in basic_preprocessed_train_corpus:\n",
        "    #tagged=okt.pos(tagged)\n",
        "    \n",
        "    #temp = []\n",
        "    #for tag in tagged:\n",
        "    #    if tag[0] in stopwords or tag[1] not in [\"Alpha\", \"Noun\", \"Foreign\"]:\n",
        "    #        continue\n",
        "    #    temp.append(tag[0])\n",
        "\n",
        "    removed_stopword_train_corpus.append(tagged)\n",
        "    \n",
        "for tagged in basic_preprocessed_test_corpus:\n",
        "    #tagged=okt.pos(tagged)\n",
        "    \n",
        "    #temp = []\n",
        "    #for tag in tagged:\n",
        "    #    if tag[0] in stopwords or tag[1] not in [\"Alpha\", \"Noun\", \"Foreign\"]:\n",
        "    #        continue\n",
        "    #    temp.append(tag[0])\n",
        "\n",
        "    removed_stopword_test_corpus.append(tagged)\n",
        "\n",
        "\n",
        "train_text = removed_stopword_train_corpus\n",
        "test_text = removed_stopword_test_corpus\n",
        "train_label = np.asarray(train.topic_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3EDAlIdtyoA"
      },
      "source": [
        "train['clear_title'] = train_text\n",
        "test['clear_title'] = test_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NcuwinZvEyo",
        "outputId": "5064102f-e844-4ec0-ef2e-787218061adb"
      },
      "source": [
        "train_length = train['clear_title'].astype(str).apply(len)\n",
        "train_length.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "196"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sab4qdXYvF1V",
        "outputId": "856b3d86-0bae-4e0e-ae8e-78c5e0bea70b"
      },
      "source": [
        "train_data_text = list(train['title'])\n",
        "\n",
        "train_clear_text = []\n",
        "\n",
        "for i in tqdm(range(len(train_data_text))):\n",
        "  train_clear_text.append(str(train_data_text[i]).replace('\\\\n', ''))\n",
        "train['clear_title'] = train_clear_text\n",
        "\n",
        "\n",
        "train_clear_text = list(train['clear_title'])\n",
        "\n",
        "train_clear_text2 = []\n",
        "\n",
        "for text in train_clear_text:\n",
        "  temp = re.sub('[-=+,#:;//●<>▲\\?:^$.☆!★()Ⅰ@*\\\"※~>`\\'…》→←]', ' ', text)\n",
        "  train_clear_text2.append(temp)\n",
        "train['clear_title'] = train_clear_text2\n",
        "\n",
        "\n",
        "test_data_text = list(test['title'])\n",
        "\n",
        "test_clear_text = []\n",
        "\n",
        "for i in tqdm(range(len(test_data_text))):\n",
        "  test_clear_text.append(test_data_text[i].replace('\\\\n', ' '))\n",
        "test['clear_title'] = test_clear_text\n",
        "\n",
        "\n",
        "test_clear_text = list(test['clear_title'])\n",
        "\n",
        "test_clear_text_final = []\n",
        "\n",
        "for text in test_clear_text:\n",
        "  temp = re.sub('[-=+,#:;//●<>▲\\?:^$.☆!★()Ⅰ@*\\\"※~>`\\'…》→←]', ' ', text)\n",
        "  test_clear_text_final.append(temp)\n",
        "test['clear_title'] = test_clear_text_final"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 773157/773157 [00:00<00:00, 1220455.14it/s]\n",
            "100%|██████████| 9131/9131 [00:00<00:00, 1396877.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcBLPSZavHKb",
        "outputId": "bf95e924-3a7d-40b7-ab41-1749617bb5f3"
      },
      "source": [
        "tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'KoBertTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToLzai_pvIkb"
      },
      "source": [
        "model_name = 'monologg/kobert'\n",
        "SEED_NUM = 615\n",
        "tf.random.set_seed(SEED_NUM)\n",
        "np.random.seed(SEED_NUM)\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 10\n",
        "VALID_SPLIT = 0.2\n",
        "MAX_LEN = 30\n",
        "NUM_CLASS = 7\n",
        "K_SPLIT = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt1adHjxvyrC"
      },
      "source": [
        "def bert_tokenizer(sent, MAX_LEN):\n",
        "    \n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "        \n",
        "        text = sent,\n",
        "        add_special_tokens = True,\n",
        "        max_length = MAX_LEN,\n",
        "        padding = True,                                   \n",
        "        return_attention_mask = True,\n",
        "        truncation = True \n",
        "    )\n",
        "\n",
        "\n",
        "    input_id = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask']\n",
        "    token_type_id = encoded_dict['token_type_ids']\n",
        "\n",
        "\n",
        "    return input_id, attention_mask, token_type_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jsRakAVwAX6",
        "outputId": "cb8deb5b-c184-4dbb-8966-95da65ab0b51"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "token_type_ids = []\n",
        "train_data_labels = []\n",
        "\n",
        "\n",
        "for train_sent, train_label in tqdm(zip(train[\"clear_title\"], train[\"topic_idx\"])): \n",
        "    try:\n",
        "\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        train_data_labels.append(train_label)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(train_sent)\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "773157it [02:19, 5528.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xFNGCQow4JS"
      },
      "source": [
        "def get_numpy_from_nonfixed_2d_array(aa, fixed_length, padding_value=0):\n",
        "    rows = []\n",
        "    for a in aa:\n",
        "        rows.append(np.pad(a, (0, fixed_length), 'constant', constant_values=padding_value)[:fixed_length])\n",
        "    return np.concatenate(rows, axis=0).reshape(-1, fixed_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F0v9ksqw72r"
      },
      "source": [
        "train_news_input_ids = get_numpy_from_nonfixed_2d_array(input_ids, fixed_length=MAX_LEN, padding_value=0)\n",
        "train_news_attention_masks = get_numpy_from_nonfixed_2d_array(attention_masks, fixed_length=MAX_LEN, padding_value=0)\n",
        "train_news_type_ids = get_numpy_from_nonfixed_2d_array(token_type_ids, fixed_length=MAX_LEN, padding_value=0)\n",
        "\n",
        "\n",
        "train_news_inputs = (train_news_input_ids, train_news_attention_masks, train_news_type_ids)\n",
        "train_data_labels = np.asarray(train_data_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "d68bee92a07c43a699270d451a58d1dc",
            "0d974aa0b5a246d185180dd82f0f4aaf",
            "0e9e4de64f1b4fce8700994ed47948eb",
            "fae2cc7139f3424eaa9a994ff831f21a",
            "2dd92274974f42159f8db80bce0fa603",
            "4c5589dda75d4ae1b84ef4c33ae2f739",
            "95cda5160fe3482eb03c4e8a6da0b935",
            "c71ac1f73c0942bcad494c8332eb9035",
            "828e4d507a6a4233976253de763d9361",
            "c5c953d47b7e47da8c5fb58779dd7dce",
            "3be1649751c8410fb49700f29128a43f"
          ]
        },
        "id": "yCdmmmeOxEyn",
        "outputId": "08063899-919a-4699-8828-9bdb098856cc"
      },
      "source": [
        "class TFBertClassifier(tf.keras.Model):                                                \n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertClassifier, self).__init__()\n",
        "\n",
        "         \n",
        "        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True) \n",
        "                                                                                                                                    \n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        # self.classifier을 통해 topic_idx를 전부 분류\n",
        "        self.classifier = tf.keras.layers.Dense(num_class,\n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"classifier\") \n",
        "\n",
        "\n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False): \n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "cls_model = TFBertClassifier(model_name=model_name, dir_path='bert_ckpt',num_class=NUM_CLASS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d68bee92a07c43a699270d451a58d1dc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/369M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertModel.\n",
            "\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTdXoUgQxIAw"
      },
      "source": [
        "optimizer = tfa.optimizers.RectifiedAdam(learning_rate=7.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-07, clipnorm=1.0)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "cls_model.compile(optimizer=optimizer,\n",
        "                                loss=loss,\n",
        "                                metrics=[metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqhCcDlUxIfJ",
        "outputId": "a5936b50-3219-4479-cf49-8632192dea57"
      },
      "source": [
        "es_callback = EarlyStopping(monitor='val_loss', \n",
        "                                mode='min',\n",
        "                                min_delta=0.0001, \n",
        "                                patience=3,\n",
        "                                baseline=0.4\n",
        "                                 ) \n",
        "\n",
        "DATA_OUT_PATH = 'kobert/best_model'\n",
        "checkpoint_path = DATA_OUT_PATH +  '/best_modeling.ckpt'\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "\n",
        "  \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, \n",
        "    monitor='val_accuracy',\n",
        "    verbose=1, \n",
        "    save_best_only=True, \n",
        "    save_weights_only=True \n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kobert/best_model -- Folder create complete \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHd7tGDwxK2G",
        "outputId": "24377bdc-5f3a-4d7f-db44-592f93447247"
      },
      "source": [
        "history = cls_model.fit(train_news_inputs, train_data_labels, \n",
        "                        epochs=NUM_EPOCHS,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        validation_split = VALID_SPLIT,\n",
        "                        callbacks=[es_callback, cp_callback]\n",
        "                        )  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f693cff10c0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f693cff10c0>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f6958203950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function wrap at 0x7f6958203950> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            " 724/4833 [===>..........................] - ETA: 32:03:16 - loss: 1.1673 - accuracy: 0.5722"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfkF1DTtxtL4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}